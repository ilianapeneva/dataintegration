# R script of BayesCluster (3X continuous)  + simulated annealing

library(mvnfast)
library(pcaMethods)
library(mvtnorm)


##-------------------------------------------------
## BAYESCLUSTER FOR 3 CONT DATASETS ---------------
##-------------------------------------------------

BayesClusternoNLP_onlySA3ds <- function(dataMatrix1, dataMatrix2, dataMatrix3, nPcomponents, nClusters, start_temp, C_k, alpha, nIterations, tol = 0.00001){
  nDataItems  <- nrow(dataMatrix1)                                           # number of data points we have
  nFeatures1  <- ncol(dataMatrix1)                                           # dimension of data 1
  nFeatures2  <- ncol(dataMatrix2)                                           # dimension of data 2
  nFeatures3  <- ncol(dataMatrix3)                                           # dimension of data 3
  Cind        <- matrix(data = NA, nrow = nDataItems, ncol = nIterations)    # cluster allocation matrix
  
  #Cind[,1] <- sample(nClusters, nDataItems,replace = TRUE)                 # randomly assign data points to clusters
  Cind[,1] <- kmeans(dataMatrix3, centers = nClusters)$cluster
 
  mean.ppca<- matrix(data = NA, nrow = nDataItems, ncol = nPcomponents)    # matrix to store the means of the clusters
  cov.ppca <- list()                                                       # list to store the covariance matrices of the clusters
  nClPoints<- c()                                                          # empty vector to store the number of points in each cluster
  max.logposterior <- -Inf                                                 # the highest log posterior - initialised to be 0
  logposterior     <- c()                                                  # the log posterior at the current iteration
  max.partition    <- c()                                                  # keep the partition which corresponds to the highest log posterior
  
  ### INITIALISATION ####
  dm1.ppca <- pca(dataMatrix1, nPcs = nPcomponents, method = "ppca", scale = "none", center = FALSE)
  #z        <- dm1.ppca@scores
  #z1       <- dm1.ppca@scores
  W1       <- dm1.ppca@loadings
  sigma2.1 <- rgamma(1,1,1)
  C.1      <- W1%*%t(W1) + sigma2.1*diag(1,nFeatures1)
  
  dm2.ppca <- pca(dataMatrix2, nPcs = nPcomponents, method = "ppca", scale = "none", center = FALSE)
  #z        <- dm2.ppca@scores
  W2       <- dm2.ppca@loadings
  sigma2.2 <- rgamma(1,1,1)
  C.2      <- W2%*%t(W2) + sigma2.2*diag(1,nFeatures2)
  
  
  dm3.ppca <- pca(dataMatrix3, nPcs = nPcomponents, method = "ppca", scale = "none", center = FALSE)
  z        <- dm3.ppca@scores
  W3       <- dm3.ppca@loadings
  sigma2.3 <- rgamma(1,1,1)
  C.3      <- W3%*%t(W3) + sigma2.3*diag(1,nFeatures3)
  
  
  
  current_temp    <- c()     # saving the cooling temperature
  current_temp[1] <- start_temp
  
  converged <- c()         # converge indicator vector
  
  # initialise the number of points, mean.ppca and the cov.ppca list to keep them updated
  for (i in 1:nClusters){
    mean.ppca[i,]   <- jitter(colMeans(z[which(Cind[,1]==i),]))
    cov.ppca[[i]]   <- jitter(cov(z[which(Cind[,1]==i),]))
    nClPoints[i]    <- nrow(z[which(Cind[,1]==i),])
  }
  
  # lists/matrices to store the MH updates
  z.chain       <- matrix(data = NA, nrow = nIterations*nDataItems, ncol = nPcomponents)     # latent variables
  mean.chain    <- matrix(data = NA, nrow = nIterations*nDataItems, ncol = nPcomponents)     # cluster means
  W1.chain      <- list()     # loadings 
  sigma21.chain <- c()        # error terms 
  
  W2.chain      <- list()
  sigma22.chain <- c()
  
  
  W3.chain      <- list()
  sigma23.chain <- c()
  
  # fill in the MH chains
  z.chain[1:nDataItems,]     <- z
  mean.chain[1:nDataItems,]  <- mean.ppca[1:nDataItems,]
  W1.chain[[1]]              <- W1
  sigma21.chain[1]           <- sigma2.1
  W2.chain[[1]]              <- W2
  sigma22.chain[1]           <- sigma2.2
  W3.chain[[1]]              <- W3
  sigma23.chain[1]           <- sigma2.3
 
  for (i in 2:nIterations){
    #i <- 2 # initialise the iteration counter
    #while((!converged) & (i < nIterations)) { 
    cat("Starting sample ", i, "...", fill=TRUE)
    
    # Step 1: sample a random permutation of {1,..., nDataItems} 
    tau <- sample(1:nDataItems, nDataItems, replace = FALSE)
    
    
    # Step 2: resample the cluster indicators
    for (j in tau){
      # remove the sufficient statistics of z[j,] from the old cluster + change the number of points in the cluster from which we remove z[j,]
      if (nClPoints[Cind[j,(i-1)]]>2){
        cov.ppca[[Cind[j,i-1]]]  <- cov.new(cov.ppca[[Cind[j,i-1]]], mean.ppca[Cind[j,i-1],], nClPoints[Cind[j,i-1]], z[j,])
        mean.ppca[Cind[j,i-1],]  <- mean.new(mean.ppca[Cind[j,i-1],],nClPoints[Cind[j,i-1]], z[j,])
        nClPoints[Cind[j,i-1]]   <- nClPoints[Cind[j,i-1]] - 1
      } else {   
        if (nClPoints[Cind[j,i-1]]==1){
          nClPoints[Cind[j,i-1]] <- nClPoints[Cind[j,i-1]] - 1
          mean.ppca[Cind[j,i-1],]<- rep(NA, nPcomponents)
          cov.ppca[[Cind[j,i-1]]]<- NA
        } 
        else { # if there are two data points
          if (nClPoints[Cind[j,i-1]]==2){
            cov.ppca[[Cind[j,i-1]]]  <- diag(1,nPcomponents)
            mean.ppca[Cind[j,i-1],]  <- mean.new(mean.ppca[Cind[j,i-1],],nClPoints[Cind[j,i-1]], z[j,])
            nClPoints[Cind[j,i-1]]   <- nClPoints[Cind[j,i-1]] - 1
          }
        } 
      }
      
      # create empty vectors for the prior and predictive probabilities of joining an existent cluster
      logprior.ex     <- rep(NA, length(nClPoints))
      pr.loglikelihood<- rep(NA, length(nClPoints))
      
      for (k in which(nClPoints>0)) {
        
        # compute the predictive likelihood of joining existent cluster 
        ppca.part <-  dmvnorm(dataMatrix1[j,], z[j,]%*%t(W1), sigma = sigma2.1*diag(1,nFeatures1), log = TRUE)+dmvnorm(dataMatrix2[j,], z[j,]%*%t(W2), sigma = sigma2.2*diag(1,nFeatures2), log = TRUE)+dmvnorm(dataMatrix3[j,], z[j,]%*%t(W3), sigma = sigma2.3*diag(1,nFeatures3), log = TRUE)
        
        pr.loglikelihood[k]    <- dmvnorm(z[j,], mean.ppca[k,] , sigma = diag(1,nPcomponents), log = TRUE)+ppca.part
        
        # compute the prior of joining existent clusters
        logprior.ex[k]       <- log(nClPoints[k]/(alpha+nDataItems-1))
      }   
      
      # create an empty vector to store posterior probabilities
      logposterior.prob <- rep(NA, length(nClPoints)+1)
      
      # compute the predictive likelihood of joining a new cluster
      prnew.loglikelihood <- dmvnorm(z[j,], rep(0,nPcomponents), sigma = diag(1,nPcomponents), log = TRUE)+ ppca.part
      
      # compute the prior of joining a new cluster 
      logprior.new         <- log(alpha/(alpha+nDataItems-1))
      
      # normalise p(c_i|.)  
      for (m in which(nClPoints>0)){
        logposterior.prob[m]<- logprior.ex[m] + pr.loglikelihood[m]
      }
      
      logposterior.prob[length(nClPoints)+1] <- logprior.new + prnew.loglikelihood
    
      # sample from the posterior
      Cind[j,i]  <- which.max(logposterior.prob)
      
      if (Cind[j,i]==(length(nClPoints)+1)) {         # that's the increased number of clusters 
        Cind[j,i] = which.min(c(nClPoints, 0))  # add it to the first cluster without any data points
        
        # if we start a new cluster, the mean = observation, cov = identity matrix, nClPoints = 1
        cov.ppca[[Cind[j,i]]]   <- diag(1, nPcomponents)
        mean.ppca[Cind[j,i],]   <- z[j,]
        nClPoints[Cind[j,i]]    <- 1
      } else {
        # add z[j,] sufficient statistics for an existing cluster + increase the number of data points in the corresponding cluster
        cov.ppca[[Cind[j,i]]] <- cov.add(cov.ppca[[Cind[j,i]]], mean.ppca[Cind[j,i],], nClPoints[Cind[j,i]], z[j,])
        mean.ppca[Cind[j,i],] <- mean.add(mean.ppca[Cind[j,i],], nClPoints[Cind[j,i]], z[j,])
        nClPoints[Cind[j,i]]  <- nClPoints[Cind[j,i]] + 1
      }
    }
    
    
    
    clustersWithPoints = which(nClPoints>0)
    current_temp[i] <- current_temp[i-1]*C_k
    #current_temp[i] <- 1/log(i)   # logarithmic
    #current_temp[i]  <- 0.95^i    # exponential
    
    prop.update <- MH_cont_SA3ds(z, current_temp[i],  dataMatrix1, dataMatrix2, dataMatrix3, nPcomponents,Cind[,i], clustersWithPoints, W1, W2, W3, sigma2.1, sigma2.2, sigma2.3, C.1, C.2, C.3, length(clustersWithPoints), mean.ppca[clustersWithPoints,], alpha, nClPoints[clustersWithPoints])
    z           <- prop.update$zs
    W1          <- prop.update$W1
    W2          <- prop.update$W2
    W3          <- prop.update$W3
    mean.ppca[clustersWithPoints,] <- prop.update$means
    sigma2.1    <- prop.update$sigma2.1
    sigma2.2    <- prop.update$sigma2.2
    sigma2.3    <- prop.update$sigma2.3
 
    
    
    # record the updates of the parameters
    z.chain[(nDataItems*(i-1)+1):(nDataItems*i),]    <- z
    mean.chain[(nDataItems*(i-1)+1):(nDataItems*i),] <- mean.ppca
    W1.chain[[i-1]]     <- W1
    W2.chain[[i-1]]     <- W2
    W3.chain[[i-1]]     <- W3
    sigma21.chain[i-1]  <- sigma2.1
    sigma22.chain[i-1]  <- sigma2.2
    sigma23.chain[i-1]  <- sigma2.3
  
    clustersWithPoints = which(nClPoints>0, arr.ind = TRUE)
    
    
    
    
    logposterior[i-1] <- logposterior_BC_DI_3ds(dataMatrix1, dataMatrix2, dataMatrix3, nPcomponents, z, Cind[,i], clustersWithPoints, W1, W2, W3, sigma2.1, sigma2.2, sigma2.3, C.1, C.2, C.3, length(clustersWithPoints), mean.ppca[clustersWithPoints,],  alpha, nClPoints[clustersWithPoints])
    if (logposterior[i-1]>= max.logposterior){
      max.logposterior <- logposterior[i-1]
      max.partition    <- Cind[,i]
    }
    
    converged[i-1] = abs(logposterior[i-2] - logposterior[i-1]) <= tol
    
    
  
    
  }
  
  out = list(clusters = Cind, cluster_points = nClPoints, number_clusters = length(which(nClPoints!=0)), best_partition = max.partition, max_logposterior = max.logposterior, logposteriors= logposterior, chainz = z.chain,  temperatures = current_temp, chaineps1 = sigma21.chain, chaineps2 = sigma22.chain, chaineps3 = sigma23.chain, convergence = converged)
  
}
