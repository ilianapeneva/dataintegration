# R script of BayesCluster (continuous) for data integration of 2 continous datasets

library(mvnfast)
library(pcaMethods)
library(mvtnorm)


##-------------------------------------------------
## BAYESCLUSTER WITH NLPs -------------------------
##-------------------------------------------------

BayesClusternoNLP_onlySA2ds_earlystop <- function(dataMatrix1, dataMatrix2, nPcomponents, nClusters, start_temp, C_k, alpha, nIterations, tol = 1e-5){
  nDataItems  <- nrow(dataMatrix1)                                           # number of data points we have
  nFeatures1  <- ncol(dataMatrix1)                                           # dimension of data 1
  nFeatures2  <- ncol(dataMatrix2)                                           # dimension of data 2
  Cind        <- matrix(data = NA, nrow = nDataItems, ncol = nIterations)    # cluster allocation matrix
  
  #Cind[,1] <- sample(nClusters, nDataItems,replace = TRUE)                 # randomly assign data points to clusters
  Cind[,1] <- kmeans(dataMatrix1, centers = nClusters)$cluster              # k-means initialisation
  print(Cind[,1])
  mean.ppca<- matrix(data = NA, nrow = nDataItems, ncol = nPcomponents)    # matrix to store the means of the clusters
  cov.ppca <- list()                                                       # list to store the covariance matrices of the clusters
  nClPoints<- c()                                                          # empty vector to store the number of points in each cluster
  max.logposterior <- -Inf                                                 # the highest log posterior - initialised to be 0
  logposterior     <- c()                                                  # the log posterior at the current iteration
  max.partition    <- c()                                                  # keep the partition which corresponds to the highest log posterior
  
  logposterior[1] <- -Inf
  
  ### INITIALISATION ####
  dm1.ppca <- pca(dataMatrix1, nPcs = nPcomponents, method = "ppca", scale = "none", center = FALSE)
  z        <- dm1.ppca@scores                    # initialise latent variables with first dataset
  W1       <- dm1.ppca@loadings
  sigma2.1 <- rgamma(1,1,1)
  C.1      <- W1%*%t(W1) + sigma2.1*diag(1,nFeatures1)
  
  dm2.ppca <- pca(dataMatrix2, nPcs = nPcomponents, method = "ppca", scale = "none", center = FALSE)
  #z        <- dm2.ppca@scores
  W2       <- dm2.ppca@loadings
  sigma2.2 <- rgamma(1,1,1)
  C.2      <- W2%*%t(W2) + sigma2.2*diag(1,nFeatures2)
    
  current_temp    <- c()     # saving the cooling temperature
  current_temp[1] <- start_temp
  
  converged  <- FALSE    # converge indicator
  
  # initialise the number of points, mean.ppca and the cov.ppca list to keep them updated
  for (i in 1:nClusters){
    mean.ppca[i,]   <- colMeans(z[which(Cind[,1]==i),])
    cov.ppca[[i]]   <- cov(z[which(Cind[,1]==i),])
    nClPoints[i]    <- nrow(z[which(Cind[,1]==i),])
  }
  
  # lists/matrices to store the MH updates
  z.chain       <- matrix(data = NA, nrow = nIterations*nDataItems, ncol = nPcomponents)     # latent variables
  mean.chain    <- matrix(data = NA, nrow = nIterations*nDataItems, ncol = nPcomponents)     # cluster means
  W1.chain      <- list()     # loadings 
  sigma21.chain <- c()        # error terms 
  
  W2.chain      <- list()
  sigma22.chain <- c()
  
  # fill in the MH chains
  z.chain[1:nDataItems,]     <- z
  mean.chain[1:nDataItems,]  <- mean.ppca[1:nDataItems,]
  W1.chain[[1]]              <- W1
  sigma21.chain[1]           <- sigma2.1
  W2.chain[[1]]              <- W2
  sigma22.chain[1]           <- sigma2.2

  i <- 2                   # initialise the iteration counter
    while((!converged) & (i < nIterations)) { 
    cat("Starting sample ", i, "...", fill=TRUE)
    
    # Step 1: sample a random permutation of {1,..., nDataItems} 
    tau <- sample(1:nDataItems, nDataItems, replace = FALSE)
    
    
    # Step 2: resample the cluster indicators
    for (j in tau){
      # remove the sufficient statistics of z[j,] from the old cluster + change the number of points in the cluster from which we remove z[j,]
      if (nClPoints[Cind[j,(i-1)]]>2){
        cov.ppca[[Cind[j,i-1]]]  <- cov.new(cov.ppca[[Cind[j,i-1]]], mean.ppca[Cind[j,i-1],], nClPoints[Cind[j,i-1]], z[j,])
        mean.ppca[Cind[j,i-1],]  <- mean.new(mean.ppca[Cind[j,i-1],],nClPoints[Cind[j,i-1]], z[j,])
        nClPoints[Cind[j,i-1]]   <- nClPoints[Cind[j,i-1]] - 1
      } else {   
        if (nClPoints[Cind[j,i-1]]==1){
          nClPoints[Cind[j,i-1]] <- nClPoints[Cind[j,i-1]] - 1
          mean.ppca[Cind[j,i-1],]<- rep(NA, nPcomponents)
          cov.ppca[[Cind[j,i-1]]]<- NA
        } 
        else { # if there are two data points
          if (nClPoints[Cind[j,i-1]]==2){
            cov.ppca[[Cind[j,i-1]]]  <- diag(1,nPcomponents)
            mean.ppca[Cind[j,i-1],]  <- mean.new(mean.ppca[Cind[j,i-1],],nClPoints[Cind[j,i-1]], z[j,])
            nClPoints[Cind[j,i-1]]   <- nClPoints[Cind[j,i-1]] - 1
          }
        } 
      }
     
      # create empty vectors for the prior and predictive probabilities of joining an existent cluster
      logprior.ex     <- rep(NA, length(nClPoints))
      pr.loglikelihood<- rep(NA, length(nClPoints))
      
      for (k in which(nClPoints>0)) {
        
        # compute the predictive likelihood of joining existent cluster 
        pr.loglikelihood[k]    <- dmvnorm(z[j,], mean.ppca[k,] , sigma = diag(1,nPcomponents), log = TRUE)+ dmvnorm(dataMatrix1[j,], z[j,]%*%t(W1), sigma = sigma2.1*diag(1,nFeatures1), log = TRUE)+dmvnorm(dataMatrix2[j,], z[j,]%*%t(W2), sigma = sigma2.2*diag(1,nFeatures2), log = TRUE)
        
        
        # compute the prior of joining existent cluster 
        logprior.ex[k]       <- log(nClPoints[k]/(alpha+nDataItems-1))
      }   

      # create an empty vector to store posterior probabilities
      logposterior.prob <- rep(NA, length(nClPoints)+1)
      
      # compute the predictive likelihood of joining a new cluster
      prnew.loglikelihood <- dmvnorm(z[j,], rep(0,nPcomponents), sigma = diag(1,nPcomponents), log = TRUE)+dmvnorm(dataMatrix1[j,], z[j,]%*%t(W1), sigma = sigma2.1*diag(1,nFeatures1), log = TRUE)+dmvnorm(dataMatrix2[j,], z[j,]%*%t(W2), sigma = sigma2.2*diag(1,nFeatures2), log = TRUE)
      
      # compute the prior of joining a new cluster 
      logprior.new         <- log(alpha/(alpha+nDataItems-1))
      
      # normalise p(c_i|.)  
      for (m in which(nClPoints>0)){
        logposterior.prob[m]<- logprior.ex[m] + pr.loglikelihood[m]
      }
      
      logposterior.prob[length(nClPoints)+1] <- logprior.new + prnew.loglikelihood
      
     
      # sample from the posterior - add to the most likely cluster
      Cind[j,i]  <- which.max(logposterior.prob)
      
      if (Cind[j,i]==(length(nClPoints)+1)) {         # that's the increased number of clusters 
        Cind[j,i] = which.min(c(nClPoints, 0))  # add it to the first cluster without any data points
        
        # if we start a new cluster, the mean = observation, cov = identity matrix, nClPoints = 1
        cov.ppca[[Cind[j,i]]]   <- diag(1, nPcomponents)
        mean.ppca[Cind[j,i],]   <- z[j,]
        nClPoints[Cind[j,i]]    <- 1
      } else {
        # add z[j,] sufficient statistics for an existing cluster + increase the number of data points in the corresponding cluster
        cov.ppca[[Cind[j,i]]] <- cov.add(cov.ppca[[Cind[j,i]]], mean.ppca[Cind[j,i],], nClPoints[Cind[j,i]], z[j,])
        mean.ppca[Cind[j,i],] <- mean.add(mean.ppca[Cind[j,i],], nClPoints[Cind[j,i]], z[j,])
        nClPoints[Cind[j,i]]  <- nClPoints[Cind[j,i]] + 1
      }
    }
    
    
    
    clustersWithPoints = which(nClPoints>0)
    current_temp[i] <- current_temp[i-1]*C_k
    #current_temp[i] <- 1/log(i)   # logarithmic
    #current_temp[i]  <- 0.95^i    # exponential
    
    prop.update <- MH_cont_SA2ds(z, current_temp[i],  dataMatrix1, dataMatrix2, nPcomponents,Cind[,i], clustersWithPoints, W1, W2,  sigma2.1, sigma2.2, C.1, C.2, length(clustersWithPoints), mean.ppca[clustersWithPoints,], alpha, nClPoints[clustersWithPoints])
    z           <- prop.update$zs
    W1          <- prop.update$W1
    W2          <- prop.update$W2
    mean.ppca[clustersWithPoints,] <- prop.update$means
    sigma2.1    <- prop.update$sigma2.1
    sigma2.2    <- prop.update$sigma2.2
  
    
    # record the updates of the parameters
    z.chain[(nDataItems*(i-1)+1):(nDataItems*i),]    <- z
    mean.chain[(nDataItems*(i-1)+1):(nDataItems*i),] <- mean.ppca
    W1.chain[[i-1]]     <- W1
    W2.chain[[i-1]]     <- W2
    sigma21.chain[i-1]  <- sigma2.1
    sigma22.chain[i-1]  <- sigma2.2
    
    clustersWithPoints = which(nClPoints>0, arr.ind = TRUE)
    
    
    
    
    logposterior[i] <- logposterior_BC_DI_error2ds(dataMatrix1, dataMatrix2,  nPcomponents, z, Cind[,i], clustersWithPoints, W1, W2, sigma2.1, sigma2.2, C.1, C.2, length(clustersWithPoints), mean.ppca[clustersWithPoints,],  alpha, nClPoints[clustersWithPoints])
    if (logposterior[i]>= max.logposterior){
      max.logposterior <- logposterior[i]
      max.partition    <- Cind[,i]
    }
    
    #converged[i-1] = abs(logposterior[i-2] - logposterior[i-1]) <= tol
    if (i==2){
      converged =  FALSE
    }
    if (i>2){
      converged = abs(logposterior[i-1] - logposterior[i]) <= tol
    }
    
    
    i<- i+1
    
    
  }
  
  out = list(clusters = Cind, cluster_points = nClPoints, number_clusters = length(which(nClPoints!=0)), best_partition = max.partition, max_logposterior = max.logposterior, logposteriors= logposterior, chainz = z.chain,  temperatures = current_temp, chaineps1 = sigma21.chain, chaineps2 = sigma22.chain)
}
